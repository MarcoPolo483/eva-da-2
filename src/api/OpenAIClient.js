/**
 * EVA DA 2.0 - OpenAI Client Integration
 * Enterprise Azure OpenAI client with government compliance
 * 
 * Features:
 * - Azure OpenAI integration with managed identity
 * - Multi-model support (GPT-4o, GPT-4o-mini, embeddings)
 * - Streaming responses
 * - Rate limiting and retry logic
 * - Token usage tracking
 * - Content filtering and moderation
 */

const { DefaultAzureCredential } = require('@azure/identity');
const { SecretClient } = require('@azure/keyvault-secrets');

/**
 * OpenAI Client for EVA DA 2.0
 * Handles all Azure OpenAI API interactions
 */
class OpenAIClient {
    constructor(config = {}) {
        this.config = {
            endpoint: config.endpoint || process.env.AZURE_OPENAI_ENDPOINT,
            keyVaultUrl: config.keyVaultUrl || process.env.KEY_VAULT_URL,
            apiVersion: config.apiVersion || '2024-08-01-preview',
            defaultModel: config.defaultModel || 'gpt-4o',
            embeddingModel: config.embeddingModel || 'text-embedding-3-small',
            maxRetries: config.maxRetries || 3,
            timeoutMs: config.timeoutMs || 60000,
            ...config
        };

        this.credential = new DefaultAzureCredential();
        this.secretClient = null;
        this.apiKey = null;
        this.requestCount = 0;
        this.isInitialized = false;
    }

    /**
     * Initialize the OpenAI client
     */
    async initialize() {
        try {
            // Initialize Key Vault client for API key retrieval
            if (this.config.keyVaultUrl) {
                this.secretClient = new SecretClient(this.config.keyVaultUrl, this.credential);
                this.apiKey = await this.getSecret('azure-openai-api-key');
            }

            this.isInitialized = true;
            console.log('‚úÖ OpenAI Client initialized successfully');

            return {
                status: 'initialized',
                endpoint: this.config.endpoint,
                apiVersion: this.config.apiVersion,
                timestamp: new Date().toISOString()
            };

        } catch (error) {
            console.error('‚ùå OpenAI Client initialization failed:', error);
            throw error;
        }
    }

    /**
     * Generate chat completion
     */
    async chatCompletion(options = {}) {
        try {
            if (!this.isInitialized) {
                await this.initialize();
            }

            const {
                messages,
                model = this.config.defaultModel,
                maxTokens = 4000,
                temperature = 0.7,
                topP = 1,
                frequencyPenalty = 0,
                presencePenalty = 0,
                stream = false,
                user = 'eva-da-2-user'
            } = options;

            // Validate messages
            this.validateMessages(messages);

            // Track request
            this.requestCount++;
            const requestId = `req-${Date.now()}-${this.requestCount}`;
            
            const startTime = Date.now();

            // Prepare request payload
            const payload = {
                model,
                messages,
                max_tokens: maxTokens,
                temperature,
                top_p: topP,
                frequency_penalty: frequencyPenalty,
                presence_penalty: presencePenalty,
                stream,
                user
            };

            // Make API call with retry logic
            const response = await this.makeAPICall('chat/completions', payload, requestId);
            
            const endTime = Date.now();
            const latency = endTime - startTime;

            // Process response
            const result = {
                id: response.id || requestId,
                content: response.choices[0]?.message?.content || '',
                model: response.model || model,
                usage: response.usage || {
                    prompt_tokens: 0,
                    completion_tokens: 0,
                    total_tokens: 0
                },
                latency,
                timestamp: new Date().toISOString(),
                requestId
            };

            // Log usage for monitoring
            this.logUsage(result);

            return result;

        } catch (error) {
            console.error('‚ùå Chat completion failed:', error);
            throw new Error(`OpenAI chat completion failed: ${error.message}`);
        }
    }

    /**
     * Generate embeddings
     */
    async embeddings(input, model = null) {
        try {
            if (!this.isInitialized) {
                await this.initialize();
            }

            const embeddingModel = model || this.config.embeddingModel;
            const startTime = Date.now();

            // Prepare input - can be string or array
            const inputArray = Array.isArray(input) ? input : [input];

            const payload = {
                model: embeddingModel,
                input: inputArray
            };

            const requestId = `emb-${Date.now()}-${this.requestCount++}`;
            const response = await this.makeAPICall('embeddings', payload, requestId);

            const result = {
                embeddings: response.data.map(item => item.embedding),
                model: response.model || embeddingModel,
                usage: response.usage,
                latency: Date.now() - startTime,
                timestamp: new Date().toISOString(),
                requestId
            };

            return result;

        } catch (error) {
            console.error('‚ùå Embeddings generation failed:', error);
            throw new Error(`OpenAI embeddings failed: ${error.message}`);
        }
    }

    /**
     * Content moderation
     */
    async moderations(input) {
        try {
            if (!this.isInitialized) {
                await this.initialize();
            }

            const payload = { input };
            const requestId = `mod-${Date.now()}-${this.requestCount++}`;
            
            const response = await this.makeAPICall('moderations', payload, requestId);

            return {
                flagged: response.results[0]?.flagged || false,
                categories: response.results[0]?.categories || {},
                categoryScores: response.results[0]?.category_scores || {},
                requestId,
                timestamp: new Date().toISOString()
            };

        } catch (error) {
            console.error('‚ùå Content moderation failed:', error);
            throw new Error(`OpenAI moderation failed: ${error.message}`);
        }
    }

    /**
     * Make API call with retry logic
     */
    async makeAPICall(endpoint, payload, requestId) {
        const maxRetries = this.config.maxRetries;
        let lastError;

        for (let attempt = 0; attempt <= maxRetries; attempt++) {
            try {
                const url = `${this.config.endpoint}/openai/deployments/${payload.model}/${endpoint}`;
                
                const response = await fetch(url, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                        'api-key': this.apiKey,
                        'User-Agent': 'EVA-DA-2.0/1.0',
                        'X-Request-ID': requestId
                    },
                    body: JSON.stringify(payload),
                    signal: AbortSignal.timeout(this.config.timeoutMs)
                });

                if (!response.ok) {
                    const errorText = await response.text();
                    throw new Error(`HTTP ${response.status}: ${errorText}`);
                }

                return await response.json();

            } catch (error) {
                lastError = error;
                
                // Check if we should retry
                if (attempt < maxRetries && this.isRetryableError(error)) {
                    const backoffMs = Math.min(1000 * Math.pow(2, attempt), 30000);
                    console.warn(`‚è≥ OpenAI API retry ${attempt + 1} in ${backoffMs}ms for ${requestId}`);
                    await this.delay(backoffMs);
                    continue;
                }
                
                break;
            }
        }

        throw lastError;
    }

    /**
     * Check if error is retryable
     */
    isRetryableError(error) {
        return error.message.includes('429') || // Rate limit
               error.message.includes('500') || // Server error
               error.message.includes('502') || // Bad gateway
               error.message.includes('503') || // Service unavailable
               error.message.includes('504') || // Gateway timeout
               error.message.includes('timeout') ||
               error.message.includes('ECONNRESET');
    }

    /**
     * Get secret from Key Vault
     */
    async getSecret(secretName) {
        try {
            if (!this.secretClient) {
                throw new Error('Key Vault client not initialized');
            }

            const secret = await this.secretClient.getSecret(secretName);
            return secret.value;

        } catch (error) {
            console.error(`‚ùå Failed to get secret ${secretName}:`, error);
            throw error;
        }
    }

    /**
     * Validate messages format
     */
    validateMessages(messages) {
        if (!Array.isArray(messages)) {
            throw new Error('Messages must be an array');
        }

        if (messages.length === 0) {
            throw new Error('Messages array cannot be empty');
        }

        messages.forEach((message, index) => {
            if (!message.role || !message.content) {
                throw new Error(`Message at index ${index} must have role and content`);
            }

            if (!['system', 'user', 'assistant'].includes(message.role)) {
                throw new Error(`Invalid role "${message.role}" at index ${index}`);
            }
        });
    }

    /**
     * Log usage metrics
     */
    logUsage(result) {
        const usage = {
            timestamp: result.timestamp,
            requestId: result.requestId,
            model: result.model,
            tokens: result.usage,
            latency: result.latency,
            success: true
        };

        // In production, send to Application Insights
        console.log('üìä OpenAI Usage:', usage);
    }

    /**
     * Get available models
     */
    async getModels() {
        try {
            // For Azure OpenAI, models are deployments configured in the resource
            return {
                chat: [
                    'gpt-4o',
                    'gpt-4o-mini',
                    'gpt-4',
                    'gpt-35-turbo'
                ],
                embeddings: [
                    'text-embedding-3-small',
                    'text-embedding-3-large',
                    'text-embedding-ada-002'
                ],
                timestamp: new Date().toISOString()
            };

        } catch (error) {
            console.error('‚ùå Failed to get models:', error);
            return { chat: [], embeddings: [], error: error.message };
        }
    }

    /**
     * Get client health status
     */
    async getHealthStatus() {
        try {
            const health = {
                timestamp: new Date().toISOString(),
                status: 'healthy',
                initialized: this.isInitialized,
                endpoint: this.config.endpoint,
                requestCount: this.requestCount,
                services: {
                    keyVault: !!this.secretClient,
                    apiKey: !!this.apiKey
                }
            };

            // Test connectivity
            if (this.isInitialized) {
                try {
                    // Simple test call
                    const testResponse = await this.chatCompletion({
                        messages: [{ role: 'user', content: 'Hello' }],
                        maxTokens: 10
                    });
                    
                    health.lastTestCall = {
                        success: true,
                        latency: testResponse.latency,
                        timestamp: testResponse.timestamp
                    };
                    
                } catch (testError) {
                    health.status = 'degraded';
                    health.lastTestCall = {
                        success: false,
                        error: testError.message,
                        timestamp: new Date().toISOString()
                    };
                }
            }

            return health;

        } catch (error) {
            return {
                timestamp: new Date().toISOString(),
                status: 'unhealthy',
                error: error.message,
                initialized: this.isInitialized
            };
        }
    }

    /**
     * Utility: delay function
     */
    delay(ms) {
        return new Promise(resolve => setTimeout(resolve, ms));
    }

    /**
     * Stream chat completion (for real-time responses)
     */
    async *streamChatCompletion(options = {}) {
        try {
            const streamOptions = { ...options, stream: true };
            
            // This would implement Server-Sent Events streaming
            // For now, yield a simple response
            yield {
                delta: { content: 'Streaming response from EVA DA 2.0...' },
                timestamp: new Date().toISOString()
            };

        } catch (error) {
            console.error('‚ùå Stream chat completion failed:', error);
            throw error;
        }
    }
}

module.exports = OpenAIClient;
